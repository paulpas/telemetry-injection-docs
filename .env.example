# =============================================================================
# LLM Configuration
# =============================================================================

# LLM Provider: openai, anthropic, or ollama
LLM_PROVIDER=ollama

# Model Name
# - Single model: LLM_MODEL=gpt-4o
# - Multiple models for rotation (Ollama only): LLM_MODEL=cogito:8b,gpt-oss:20b,llama3
# Note: Model rotation enables parallel processing on multi-GPU systems
LLM_MODEL=codellama

# Base URL (Ollama only)
LLM_BASE_URL=http://localhost:11434/v1

# =============================================================================
# API Keys (Provider-specific)
# =============================================================================

# OpenAI API Key (required for provider=openai)
OPENAI_API_KEY=

# Anthropic API Key (required for provider=anthropic)
ANTHROPIC_API_KEY=

# =============================================================================
# Parallel Processing
# =============================================================================

# Maximum parallel workers (default: 4)
# Recommended: Number of GPUs or CPU cores
MAX_WORKERS=4

# =============================================================================
# Token Configuration (Auto-detected, optional overrides)
# =============================================================================

# Ollama Context Window (optional, default: 32768)
# Used for token limit detection
OLLAMA_NUM_CTX=32768

# =============================================================================
# Debug and Logging
# =============================================================================

# Enable Debug Mode (default: false)
# Shows additional output during execution
DEBUG=false

# Enable Debug Trace Logging (default: false)
# Logs all function calls, LLM requests/responses, and execution flow
DEBUG_TRACE=false

# Debug Trace Log Level (default: DEBUG)
# Options: TRACE (most detailed), DEBUG, INFO, WARNING, ERROR
DEBUG_TRACE_LEVEL=DEBUG

# Debug Trace Console Output (default: true)
# Print debug logs to console in addition to file
DEBUG_TRACE_CONSOLE=true

# =============================================================================
# Telemetry Receiver
# =============================================================================

# Telemetry Receiver URL (optional)
# Endpoint where instrumented code sends telemetry data
RECEIVER_URL=http://localhost:8000/telemetry

# =============================================================================
# Advanced Configuration
# =============================================================================

# Enable Linting (default: true)
# Validates generated telemetry code syntax
ENABLE_LINTING=true

# Model Blacklist Threshold (default: 3)
# Number of failures before blacklisting a model
BLACKLIST_THRESHOLD=3

# =============================================================================
# Feature Flags
# =============================================================================

# Auto Token Detection: Enabled by default
# Automatically detects max tokens via API and caches results
# Cache location: ~/.cache/telemetry_injector/token_limits.json

# VRAM-Based GPU Scheduling: Enabled by default
# Automatically detects GPU VRAM and schedules models accordingly
# Supports both AMD (ROCm) and NVIDIA (CUDA)

# Cost Tracking: Enabled by default
# Tracks API call costs (use --budget flag to set limits)

# =============================================================================
# Benchmark Mode (Advanced)
# =============================================================================

# Judge Ensemble Mode: Use --perform-benchmark flag
# Queries all models and selects highest quality response via voting
# Note: Significantly slower and more expensive than default mode
# Only use for quality benchmarking and model comparison

# =============================================================================
# Notes
# =============================================================================

# Default Execution Mode:
# - Uses Parallel Executor for speed
# - First valid response wins
# - Recommended for production use

# Benchmark Mode (--perform-benchmark):
# - Uses Judge Ensemble for quality
# - All models vote on best response
# - Recommended for testing and comparison only

# Model Rotation (Ollama):
# - Comma-separated models enable rotation
# - Example: LLM_MODEL=cogito:8b,gpt-oss:20b,llama3
# - Allows parallel processing on multi-GPU systems
# - Each request uses next model in round-robin fashion

# Token Auto-Detection:
# - Automatically detects limits via API
# - Caches results to ~/.cache/telemetry_injector/
# - Falls back to known limits if detection fails
# - Override with OLLAMA_NUM_CTX for Ollama

# GPU VRAM Scheduling:
# - Automatically detects AMD (rocm-smi) or NVIDIA (nvidia-smi)
# - Estimates model VRAM requirements from parameter count
# - Assigns models to GPUs with sufficient VRAM
# - No configuration required

# Debug Trace Logging:
# - Set DEBUG_TRACE=true to enable
# - Logs saved to logs/debug_trace_TIMESTAMP.jsonl
# - Use DEBUG_TRACE_LEVEL to control verbosity
# - Trace logs: function calls, LLM conversations, model rotation, GPU assignment

# =============================================================================
